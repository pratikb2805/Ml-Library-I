{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import warnings\n",
    "np.seterr( over='ignore' )\n",
    "\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "\n",
    "class layers:\n",
    "    def __init__(self,neu,r,c):\n",
    "        self.neu=int(neu)\n",
    "        self.wt=np.random.randn(r,c)\n",
    "        self.bias=np.random.randn(self.neu)\n",
    "\n",
    "class network:\n",
    "    def __init__(self,data_in,data_out,layers_list,l_rate=0.001):\n",
    "        self.x=np.array(data_in)\n",
    "        self.y=np.array(data_out)    \n",
    "        xx=np.array_split(self.x.T,2)\n",
    "        self.xt=xx[0].T\n",
    "        self.xc=xx[1].T\n",
    "        yy=np.array_split(self.y.T,2)\n",
    "        self.yt=yy[0].T\n",
    "        self.yc=yy[1].T \n",
    "        self.l=int(len(li))\n",
    "        self.ly=layers_list\n",
    "        self.l_rate= float(l_rate)\n",
    "        try:\n",
    "            self.n=self.xt.shape[1]\n",
    "        except:\n",
    "            self.n=self.xt.shape[0]\n",
    "        self.m=int(self.n/100)\n",
    "        self.x_split=np.array_split(self.xt.T,self.m)\n",
    "        self.y_split=np.array_split(self.yt.T,self.m)\n",
    "     \n",
    "    def sigm(self,x):\n",
    "        x = np.clip( x, -500, 500 )\n",
    "        return 1/(1+np.e**-x)\n",
    "\n",
    "\n",
    "    def der_sigm(self,x):\n",
    "        return sigm(x)/(1-sigm(x))\n",
    "\n",
    "    def forward(self):\n",
    "        for i in range(self.l):\n",
    "            self.ly[i].z=np.empty((self.ly[i].neu,self.xt.shape[1]),dtype=np.float64)\n",
    "            self.ly[i].a=np.empty((self.ly[i].neu,self.xt.shape[1]),dtype=np.float64)\n",
    "\n",
    "        for i in range(self.l):\n",
    "            if i==0:\n",
    "                for j in range(int(self.ly[i].neu)):\n",
    "                    self.ly[i].z[j,:]=np.dot(self.ly[i].wt,self.xt)[j,:] +self.ly[i].bias[j]              \n",
    "            else:\n",
    "                for j in range(int(self.ly[i].neu)):\n",
    "                    self.ly[i].z[j,:]=np.dot(self.ly[i].wt,self.ly[i-1].a)[j,:] + self.ly[i].bias[j]\n",
    "            self.ly[i].a=self.sigm(self.ly[i].z)\n",
    "    \n",
    "    def cost(self):\n",
    "        return np.sum((np.dot(self.yt,np.log(self.ly[-1].a).T))+ np.dot( 1-self.yt,np.log(1-self.ly[-1].a).T ))\n",
    "    \n",
    "    def backprop(self):\n",
    "        for i in range(self.l-1, -1, -1):\n",
    "            if i!=self.l-1:\n",
    "                self.ly[i].error=np.dot(self.ly[i+1].wt.T, self.ly[i+1].a)\n",
    "            else:\n",
    "                self.ly[i].error=self.ly[i].a-self.yt\n",
    "    \n",
    "    def update(self):\n",
    "        for i in range(self.l-1, -1, -1):\n",
    "            if i!=0:\n",
    "                temp= np.dot(self.ly[i].error,self.ly[i-1].a.T)\n",
    "            else:\n",
    "                temp= np.dot(self.ly[i].error,self.xt.T)            \n",
    "            temp_bias= np.mean(self.ly[i].error, axis=1)\n",
    "            self.ly[i].wt=self.ly[i].wt-temp*self.l_rate\n",
    "            self.ly[i].bias=self.ly[i].bias-np.ndarray.flatten(temp_bias)*self.l_rate\n",
    "\n",
    "    def train(self,epoch):\n",
    "        \n",
    "        x_split=np.array_split(self.xt, self.m+1, axis=1)\n",
    "        y_split=np.array_split(self.yt, self.m+1, axis=1)\n",
    "        for i in range(self.m+1):\n",
    "            self.xt=x_split[i]\n",
    "            self.yt=y_split[i]\n",
    "            for j in range(int(epoch)):\n",
    "                self.forward()\n",
    "                self.backprop()\n",
    "                self.update()\n",
    "\n",
    "        \n",
    "\n",
    "    def implement(self,x_data):            # just one sample at a time\n",
    "        x_data=np.array(x_data)/100\n",
    "        l=int(x_data.shape[0])\n",
    "        x_data=np.expand_dims(x_data,0)\n",
    "        x_data=x_data.T\n",
    "        self.xi=x_data\n",
    "        for i in range(self.l):\n",
    "            if i==0:\n",
    "                self.ly[i].z=np.dot(self.ly[i].wt,x_data)+self.ly[i].bias\n",
    "                self.ly[i].a=self.sigm( self.ly[i].z)\n",
    "            else:\n",
    "                self.ly[i].z=np.dot(self.ly[i].wt,self.ly[i-1].a)+self.ly[i].bias\n",
    "                self.ly[i].a=self.sigm(self.ly[i].z)\n",
    "        print(self.ly[-1].a)\n",
    "\n",
    "    def train2(self):\n",
    "        self.forward()\n",
    "        self.backprop()\n",
    "        self.update()\n",
    "        while np.mean(np.abs(self.ly[-1].error))>(5*10**-2):\n",
    "            self.forward()\n",
    "            self.backprop()\n",
    "            self.update()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data=pd.read_csv(\"mnist_train_small.csv\",header=None)\n",
    "data.dropna()\n",
    "d=np.array(data)\n",
    "y=d[:,[0]]\n",
    "x=np.delete(d,0,1)/255\n",
    "z=np.eye(10)\n",
    "yy=np.empty((y.shape[0],10),dtype=np.float64)\n",
    "for i in range(y.shape[0]):\n",
    "    for j in range(10):\n",
    "        if y[i]==j+1:\n",
    "            yy[i,:]=z[j,:]\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "li=[]\n",
    "li.append(layers(10,10,784))\n",
    "li.append(layers(10,10,10))\n",
    "li.append(layers(10,10,10))\n",
    "li.append(layers(10,10,10))\n",
    "li.append(layers(10,10,10))\n",
    "li.append(layers(10,10,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=network(x.T,y.T,li)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan]])"
      ]
<<<<<<< HEAD
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ly[0].a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(10, 0), dtype=float64)"
=======
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9bLyUGcRszjl",
        "colab": {},
        "cellView": "code"
      },
      "source": [
        "#@title Default title text\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import warnings\n",
        "\n",
        "def fxn():\n",
        "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    fxn()\n",
        "\n",
        "class layers:\n",
        "    def __init__(self,neu,r,c):\n",
        "        self.neu=int(neu)\n",
        "        self.wt=np.random.randn(r,c)\n",
        "        self.bias=np.random.randn(self.neu)\n",
        "\n",
        "class network:\n",
        "    def __init__(self,data_in,data_out,layers_list,l_rate=0.001):\n",
        "        self.x=np.array(data_in)\n",
        "        self.y=np.array(data_out)    \n",
        "        xx=np.array_split(self.x.T,2)\n",
        "        self.xt=xx[0].T\n",
        "        self.xc=xx[1].T\n",
        "        yy=np.array_split(self.y.T,2)\n",
        "        self.yt=yy[0].T\n",
        "        self.yc=yy[1].T \n",
        "        self.l=int(len(li))\n",
        "        self.ly=layers_list\n",
        "        self.l_rate= float(l_rate)\n",
        "        try:\n",
        "            self.n=self.xt.shape[1]\n",
        "        except:\n",
        "            self.n=self.xt.shape[0]\n",
        "        self.m=int(self.n/100)\n",
        "        self.x_split=np.array_split(self.xt.T,self.m)\n",
        "        self.y_split=np.array_split(self.yt.T,self.m)\n",
        "     \n",
        "    def sigm(self,x):\n",
        "        return 1/(1+np.e**-x)\n",
        "\n",
        "\n",
        "    def der_sigm(self,x):\n",
        "        return sigm(x)/(1-sigm(x))\n",
        "\n",
        "    def forward(self):\n",
        "        for i in range(self.l):\n",
        "            self.ly[i].z=np.empty((self.ly[i].neu,self.xt.shape[1]),dtype=np.float64)\n",
        "            self.ly[i].a=np.empty((self.ly[i].neu,self.xt.shape[1]),dtype=np.float64)\n",
        "\n",
        "        for i in range(self.l):\n",
        "            if i==0:\n",
        "                for j in range(int(self.ly[i].neu)):\n",
        "                    self.ly[i].z[j,:]=np.dot(self.ly[i].wt,self.xt)[j,:] +self.ly[i].bias[j]              \n",
        "            else:\n",
        "                for j in range(int(self.ly[i].neu)):\n",
        "                    self.ly[i].z[j,:]=np.dot(self.ly[i].wt,self.ly[i-1].a)[j,:] + self.ly[i].bias[j]\n",
        "            self.ly[i].a=self.sigm(self.ly[i].z)\n",
        "    \n",
        "    def cost(self):\n",
        "        return np.sum((np.dot(self.yt,np.log(self.ly[-1].a).T))+ np.dot( 1-self.yt,np.log(1-self.ly[-1].a).T ))\n",
        "    \n",
        "    def backprop(self):\n",
        "        for i in range(self.l-1, -1, -1):\n",
        "            if i!=self.l-1:\n",
        "                self.ly[i].error=np.dot(self.ly[i+1].wt.T, self.ly[i+1].a)\n",
        "            else:\n",
        "                self.ly[i].error=self.ly[i].a-self.yt\n",
        "    \n",
        "    def update(self):\n",
        "        for i in range(self.l-1, -1, -1):\n",
        "            if i!=0:\n",
        "                temp= np.dot(self.ly[i].error,self.ly[i-1].a.T)\n",
        "            else:\n",
        "                temp= np.dot(self.ly[i].error,self.xt.T)            \n",
        "            temp_bias= np.mean(self.ly[i].error, axis=1)\n",
        "            self.ly[i].wt=self.ly[i].wt-temp*self.l_rate\n",
        "            self.ly[i].bias=self.ly[i].bias-np.ndarray.flatten(temp_bias)*self.l_rate\n",
        "\n",
        "    def train(self,epoch):\n",
        "        \n",
        "        x_split=np.array_split(self.xt, self.m+1, axis=1)\n",
        "        y_split=np.array_split(self.yt, self.m+1, axis=1)\n",
        "        for i in range(self.m+1):\n",
        "            self.xt=x_split[i]\n",
        "            self.yt=y_split[i]\n",
        "            for j in range(int(epoch)):\n",
        "                self.forward()\n",
        "                self.backprop()\n",
        "                self.update()\n",
        "\n",
        "        \n",
        "\n",
        "    def implement(self,x_data):            # just one sample at a time\n",
        "        x_data=np.array(x_data)/100\n",
        "        l=int(x_data.shape[0])\n",
        "        x_data=np.expand_dims(x_data,0)\n",
        "        x_data=x_data.T\n",
        "        self.xi=x_data\n",
        "        for i in range(self.l):\n",
        "            if i==0:\n",
        "                self.ly[i].z=np.dot(self.ly[i].wt,x_data)+self.ly[i].bias\n",
        "                self.ly[i].a=self.sigm( self.ly[i].z)\n",
        "            else:\n",
        "                self.ly[i].z=np.dot(self.ly[i].wt,self.ly[i-1].a)+self.ly[i].bias\n",
        "                self.ly[i].a=self.sigm(self.ly[i].z)\n",
        "        print(self.ly[-1].a)\n",
        "\n",
        "    def train2(self):\n",
        "        self.forward()\n",
        "        self.backprop()\n",
        "        self.update()\n",
        "        while np.mean(np.abs(self.ly[-1].error))>(5*10**-2):\n",
        "            self.forward()\n",
        "            self.backprop()\n",
        "            self.update()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GYWLHPERTGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Default title text\n",
        "\n",
        "data=pd.read_csv(\"/content/sample_data/mnist_train_small.csv\",header=None)\n",
        "data.dropna()\n",
        "d=np.array(data)\n",
        "y=d[:,[0]]\n",
        "x=np.delete(d,0,1)/255\n",
        "z=np.eye(10)\n",
        "yy=np.empty((y.shape[0],10),dtype=np.float64)\n",
        "for i in range(y.shape[0]):\n",
        "    for j in range(10):\n",
        "        if y[i]==j:\n",
        "            yy[i,:]=z[j,:]\n",
        "        else:\n",
        "            pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1KzejAdRTG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "li=[]\n",
        "li.append(layers(10,10,784))\n",
        "li.append(layers(10,10,10))\n",
        "li.append(layers(10,10,10))\n",
        "li.append(layers(10,10,10))\n",
        "li.append(layers(10,10,10))\n",
        "li.append(layers(10,10,10))\n",
        "li.append(layers(10,10,10))\n",
        "li.append(layers(10,10,10))\n",
        "li.append(layers(10,10,10))\n",
        "li.append(layers(10,10,10))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKADZ6egRTHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=network(x.T,yy.T,li)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6VAStxdW70t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.forward()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C_JLBooSGG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.backprop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inna6WoUATnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.update()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETpuwjkbRTHv",
        "colab_type": "code",
        "outputId": "901f9401-f059-4481-f891-d6e3c4f9ae0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "model.ly[0].a"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.51994979e-01, 5.35509891e-01, 7.91428162e-01, ...,\n",
              "        8.77438420e-01, 9.98711038e-01, 4.05965034e-01],\n",
              "       [5.27362186e-01, 9.99810406e-01, 9.24026867e-02, ...,\n",
              "        3.26702659e-02, 9.97761304e-01, 9.84562718e-01],\n",
              "       [9.98013131e-01, 3.15782256e-03, 1.59174364e-01, ...,\n",
              "        8.08191086e-01, 9.76119581e-01, 9.36576319e-01],\n",
              "       ...,\n",
              "       [1.06095590e-04, 1.61094426e-01, 9.13440271e-02, ...,\n",
              "        1.76758454e-03, 7.76500141e-02, 2.00356098e-02],\n",
              "       [1.29041568e-01, 4.04497336e-01, 9.79197707e-01, ...,\n",
              "        2.00782699e-03, 4.39967548e-02, 3.34475710e-01],\n",
              "       [9.86217920e-01, 5.11267428e-01, 6.40892429e-02, ...,\n",
              "        9.99454255e-01, 9.98718054e-01, 9.98335525e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
>>>>>>> c3a464b9ca1a81b3c40d7b08dd361281c3101406
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ly[0].a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "data": {
      "text/plain": [
       "array([-3.4943, -3.4943, -3.4943, -3.4943, -3.4943, -3.4943, -3.4943,\n",
       "       -3.4943, -3.4943, -3.4943])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
=======
      "cell_type": "code",
      "metadata": {
        "id": "lMZ3_hDkRTHa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bfc7bae9-7c0c-4b5f-e30c-e5e5d4d5ff09"
      },
      "source": [
        "model.train(1000)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: RuntimeWarning: overflow encountered in power\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBdu-gdqxNcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=np.loadtxt(\"/content/Outlier.txt\", delimiter=',', dtype=np.float64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wxiL6VC_rtC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c70a2a0a-f560-45f0-d4dd-9170aa28e2ae"
      },
      "source": [
        "np.mean(np.abs(model.ly[-1].error))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.176757914141216"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpNNg2Jq_3Zw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fcc4cf32-c5dc-4056-d2ee-424a9440387e"
      },
      "source": [
        "model.ly[-1].a"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.08081129, 0.08081129, 0.08081129, 0.08081129, 0.08081129,\n",
              "        0.08081129, 0.08081129, 0.08081129, 0.08081129, 0.08081129,\n",
              "        0.08081129, 0.08081129, 0.08081129, 0.08081129, 0.08081129,\n",
              "        0.08081129, 0.08081129, 0.08081129, 0.08081129, 0.08081129,\n",
              "        0.08081129, 0.08081129, 0.08081129, 0.08081129, 0.08081129,\n",
              "        0.08081129, 0.08081129, 0.08081129, 0.08081129, 0.08081129,\n",
              "        0.08081129, 0.08081129, 0.08081129, 0.08081129, 0.08081129,\n",
              "        0.08081129, 0.08081129, 0.08081129, 0.08081129, 0.08081129,\n",
              "        0.08081129, 0.08081129, 0.08081129, 0.08081129, 0.08081129,\n",
              "        0.08081129, 0.08081129, 0.08081129, 0.08081129, 0.08081129,\n",
              "        0.08081129, 0.08081129, 0.08081129, 0.08081129, 0.08081129,\n",
              "        0.08081129, 0.08081129, 0.08081129, 0.08081129, 0.08081129,\n",
              "        0.08081129, 0.08081129, 0.08081129, 0.08081129, 0.08081129,\n",
              "        0.08081129, 0.08081129, 0.08081129, 0.08081129, 0.08081129,\n",
              "        0.08081129, 0.08081129, 0.08081129, 0.08081129, 0.08081129,\n",
              "        0.08081129, 0.08081129, 0.08081129, 0.08081129, 0.08081129,\n",
              "        0.08081129, 0.08081129, 0.08081129, 0.08081129, 0.08081129,\n",
              "        0.08081129, 0.08081129, 0.08081129, 0.08081129, 0.08081129,\n",
              "        0.08081129, 0.08081129, 0.08081129, 0.08081129, 0.08081129,\n",
              "        0.08081129, 0.08081129, 0.08081129, 0.08081129],\n",
              "       [0.13130895, 0.13130895, 0.13130895, 0.13130895, 0.13130895,\n",
              "        0.13130895, 0.13130895, 0.13130895, 0.13130895, 0.13130895,\n",
              "        0.13130895, 0.13130895, 0.13130895, 0.13130895, 0.13130895,\n",
              "        0.13130895, 0.13130895, 0.13130895, 0.13130895, 0.13130895,\n",
              "        0.13130895, 0.13130895, 0.13130895, 0.13130895, 0.13130895,\n",
              "        0.13130895, 0.13130895, 0.13130895, 0.13130895, 0.13130895,\n",
              "        0.13130895, 0.13130895, 0.13130895, 0.13130895, 0.13130895,\n",
              "        0.13130895, 0.13130895, 0.13130895, 0.13130895, 0.13130895,\n",
              "        0.13130895, 0.13130895, 0.13130895, 0.13130895, 0.13130895,\n",
              "        0.13130895, 0.13130895, 0.13130895, 0.13130895, 0.13130895,\n",
              "        0.13130895, 0.13130895, 0.13130895, 0.13130895, 0.13130895,\n",
              "        0.13130895, 0.13130895, 0.13130895, 0.13130895, 0.13130895,\n",
              "        0.13130895, 0.13130895, 0.13130895, 0.13130895, 0.13130895,\n",
              "        0.13130895, 0.13130895, 0.13130895, 0.13130895, 0.13130895,\n",
              "        0.13130895, 0.13130895, 0.13130895, 0.13130895, 0.13130895,\n",
              "        0.13130895, 0.13130895, 0.13130895, 0.13130895, 0.13130895,\n",
              "        0.13130895, 0.13130895, 0.13130895, 0.13130895, 0.13130895,\n",
              "        0.13130895, 0.13130895, 0.13130895, 0.13130895, 0.13130895,\n",
              "        0.13130895, 0.13130895, 0.13130895, 0.13130895, 0.13130895,\n",
              "        0.13130895, 0.13130895, 0.13130895, 0.13130895],\n",
              "       [0.10099698, 0.10099698, 0.10099698, 0.10099698, 0.10099698,\n",
              "        0.10099698, 0.10099698, 0.10099698, 0.10099698, 0.10099698,\n",
              "        0.10099698, 0.10099698, 0.10099698, 0.10099698, 0.10099698,\n",
              "        0.10099698, 0.10099698, 0.10099698, 0.10099698, 0.10099698,\n",
              "        0.10099698, 0.10099698, 0.10099698, 0.10099698, 0.10099698,\n",
              "        0.10099698, 0.10099698, 0.10099698, 0.10099698, 0.10099698,\n",
              "        0.10099698, 0.10099698, 0.10099698, 0.10099698, 0.10099698,\n",
              "        0.10099698, 0.10099698, 0.10099698, 0.10099698, 0.10099698,\n",
              "        0.10099698, 0.10099698, 0.10099698, 0.10099698, 0.10099698,\n",
              "        0.10099698, 0.10099698, 0.10099698, 0.10099698, 0.10099698,\n",
              "        0.10099698, 0.10099698, 0.10099698, 0.10099698, 0.10099698,\n",
              "        0.10099698, 0.10099698, 0.10099698, 0.10099698, 0.10099698,\n",
              "        0.10099698, 0.10099698, 0.10099698, 0.10099698, 0.10099698,\n",
              "        0.10099698, 0.10099698, 0.10099698, 0.10099698, 0.10099698,\n",
              "        0.10099698, 0.10099698, 0.10099698, 0.10099698, 0.10099698,\n",
              "        0.10099698, 0.10099698, 0.10099698, 0.10099698, 0.10099698,\n",
              "        0.10099698, 0.10099698, 0.10099698, 0.10099698, 0.10099698,\n",
              "        0.10099698, 0.10099698, 0.10099698, 0.10099698, 0.10099698,\n",
              "        0.10099698, 0.10099698, 0.10099698, 0.10099698, 0.10099698,\n",
              "        0.10099698, 0.10099698, 0.10099698, 0.10099698],\n",
              "       [0.11110061, 0.11110061, 0.11110061, 0.11110061, 0.11110061,\n",
              "        0.11110061, 0.11110061, 0.11110061, 0.11110061, 0.11110061,\n",
              "        0.11110061, 0.11110061, 0.11110061, 0.11110061, 0.11110061,\n",
              "        0.11110061, 0.11110061, 0.11110061, 0.11110061, 0.11110061,\n",
              "        0.11110061, 0.11110061, 0.11110061, 0.11110061, 0.11110061,\n",
              "        0.11110061, 0.11110061, 0.11110061, 0.11110061, 0.11110061,\n",
              "        0.11110061, 0.11110061, 0.11110061, 0.11110061, 0.11110061,\n",
              "        0.11110061, 0.11110061, 0.11110061, 0.11110061, 0.11110061,\n",
              "        0.11110061, 0.11110061, 0.11110061, 0.11110061, 0.11110061,\n",
              "        0.11110061, 0.11110061, 0.11110061, 0.11110061, 0.11110061,\n",
              "        0.11110061, 0.11110061, 0.11110061, 0.11110061, 0.11110061,\n",
              "        0.11110061, 0.11110061, 0.11110061, 0.11110061, 0.11110061,\n",
              "        0.11110061, 0.11110061, 0.11110061, 0.11110061, 0.11110061,\n",
              "        0.11110061, 0.11110061, 0.11110061, 0.11110061, 0.11110061,\n",
              "        0.11110061, 0.11110061, 0.11110061, 0.11110061, 0.11110061,\n",
              "        0.11110061, 0.11110061, 0.11110061, 0.11110061, 0.11110061,\n",
              "        0.11110061, 0.11110061, 0.11110061, 0.11110061, 0.11110061,\n",
              "        0.11110061, 0.11110061, 0.11110061, 0.11110061, 0.11110061,\n",
              "        0.11110061, 0.11110061, 0.11110061, 0.11110061, 0.11110061,\n",
              "        0.11110061, 0.11110061, 0.11110061, 0.11110061],\n",
              "       [0.10101849, 0.10101849, 0.10101849, 0.10101849, 0.10101849,\n",
              "        0.10101849, 0.10101849, 0.10101849, 0.10101849, 0.10101849,\n",
              "        0.10101849, 0.10101849, 0.10101849, 0.10101849, 0.10101849,\n",
              "        0.10101849, 0.10101849, 0.10101849, 0.10101849, 0.10101849,\n",
              "        0.10101849, 0.10101849, 0.10101849, 0.10101849, 0.10101849,\n",
              "        0.10101849, 0.10101849, 0.10101849, 0.10101849, 0.10101849,\n",
              "        0.10101849, 0.10101849, 0.10101849, 0.10101849, 0.10101849,\n",
              "        0.10101849, 0.10101849, 0.10101849, 0.10101849, 0.10101849,\n",
              "        0.10101849, 0.10101849, 0.10101849, 0.10101849, 0.10101849,\n",
              "        0.10101849, 0.10101849, 0.10101849, 0.10101849, 0.10101849,\n",
              "        0.10101849, 0.10101849, 0.10101849, 0.10101849, 0.10101849,\n",
              "        0.10101849, 0.10101849, 0.10101849, 0.10101849, 0.10101849,\n",
              "        0.10101849, 0.10101849, 0.10101849, 0.10101849, 0.10101849,\n",
              "        0.10101849, 0.10101849, 0.10101849, 0.10101849, 0.10101849,\n",
              "        0.10101849, 0.10101849, 0.10101849, 0.10101849, 0.10101849,\n",
              "        0.10101849, 0.10101849, 0.10101849, 0.10101849, 0.10101849,\n",
              "        0.10101849, 0.10101849, 0.10101849, 0.10101849, 0.10101849,\n",
              "        0.10101849, 0.10101849, 0.10101849, 0.10101849, 0.10101849,\n",
              "        0.10101849, 0.10101849, 0.10101849, 0.10101849, 0.10101849,\n",
              "        0.10101849, 0.10101849, 0.10101849, 0.10101849],\n",
              "       [0.06062113, 0.06062113, 0.06062113, 0.06062113, 0.06062113,\n",
              "        0.06062113, 0.06062113, 0.06062113, 0.06062113, 0.06062113,\n",
              "        0.06062113, 0.06062113, 0.06062113, 0.06062113, 0.06062113,\n",
              "        0.06062113, 0.06062113, 0.06062113, 0.06062113, 0.06062113,\n",
              "        0.06062113, 0.06062113, 0.06062113, 0.06062113, 0.06062113,\n",
              "        0.06062113, 0.06062113, 0.06062113, 0.06062113, 0.06062113,\n",
              "        0.06062113, 0.06062113, 0.06062113, 0.06062113, 0.06062113,\n",
              "        0.06062113, 0.06062113, 0.06062113, 0.06062113, 0.06062113,\n",
              "        0.06062113, 0.06062113, 0.06062113, 0.06062113, 0.06062113,\n",
              "        0.06062113, 0.06062113, 0.06062113, 0.06062113, 0.06062113,\n",
              "        0.06062113, 0.06062113, 0.06062113, 0.06062113, 0.06062113,\n",
              "        0.06062113, 0.06062113, 0.06062113, 0.06062113, 0.06062113,\n",
              "        0.06062113, 0.06062113, 0.06062113, 0.06062113, 0.06062113,\n",
              "        0.06062113, 0.06062113, 0.06062113, 0.06062113, 0.06062113,\n",
              "        0.06062113, 0.06062113, 0.06062113, 0.06062113, 0.06062113,\n",
              "        0.06062113, 0.06062113, 0.06062113, 0.06062113, 0.06062113,\n",
              "        0.06062113, 0.06062113, 0.06062113, 0.06062113, 0.06062113,\n",
              "        0.06062113, 0.06062113, 0.06062113, 0.06062113, 0.06062113,\n",
              "        0.06062113, 0.06062113, 0.06062113, 0.06062113, 0.06062113,\n",
              "        0.06062113, 0.06062113, 0.06062113, 0.06062113],\n",
              "       [0.08079908, 0.08079908, 0.08079908, 0.08079908, 0.08079908,\n",
              "        0.08079908, 0.08079908, 0.08079908, 0.08079908, 0.08079908,\n",
              "        0.08079908, 0.08079908, 0.08079908, 0.08079908, 0.08079908,\n",
              "        0.08079908, 0.08079908, 0.08079908, 0.08079908, 0.08079908,\n",
              "        0.08079908, 0.08079908, 0.08079908, 0.08079908, 0.08079908,\n",
              "        0.08079908, 0.08079908, 0.08079908, 0.08079908, 0.08079908,\n",
              "        0.08079908, 0.08079908, 0.08079908, 0.08079908, 0.08079908,\n",
              "        0.08079908, 0.08079908, 0.08079908, 0.08079908, 0.08079908,\n",
              "        0.08079908, 0.08079908, 0.08079908, 0.08079908, 0.08079908,\n",
              "        0.08079908, 0.08079908, 0.08079908, 0.08079908, 0.08079908,\n",
              "        0.08079908, 0.08079908, 0.08079908, 0.08079908, 0.08079908,\n",
              "        0.08079908, 0.08079908, 0.08079908, 0.08079908, 0.08079908,\n",
              "        0.08079908, 0.08079908, 0.08079908, 0.08079908, 0.08079908,\n",
              "        0.08079908, 0.08079908, 0.08079908, 0.08079908, 0.08079908,\n",
              "        0.08079908, 0.08079908, 0.08079908, 0.08079908, 0.08079908,\n",
              "        0.08079908, 0.08079908, 0.08079908, 0.08079908, 0.08079908,\n",
              "        0.08079908, 0.08079908, 0.08079908, 0.08079908, 0.08079908,\n",
              "        0.08079908, 0.08079908, 0.08079908, 0.08079908, 0.08079908,\n",
              "        0.08079908, 0.08079908, 0.08079908, 0.08079908, 0.08079908,\n",
              "        0.08079908, 0.08079908, 0.08079908, 0.08079908],\n",
              "       [0.20199494, 0.20199494, 0.20199494, 0.20199494, 0.20199494,\n",
              "        0.20199494, 0.20199494, 0.20199494, 0.20199494, 0.20199494,\n",
              "        0.20199494, 0.20199494, 0.20199494, 0.20199494, 0.20199494,\n",
              "        0.20199494, 0.20199494, 0.20199494, 0.20199494, 0.20199494,\n",
              "        0.20199494, 0.20199494, 0.20199494, 0.20199494, 0.20199494,\n",
              "        0.20199494, 0.20199494, 0.20199494, 0.20199494, 0.20199494,\n",
              "        0.20199494, 0.20199494, 0.20199494, 0.20199494, 0.20199494,\n",
              "        0.20199494, 0.20199494, 0.20199494, 0.20199494, 0.20199494,\n",
              "        0.20199494, 0.20199494, 0.20199494, 0.20199494, 0.20199494,\n",
              "        0.20199494, 0.20199494, 0.20199494, 0.20199494, 0.20199494,\n",
              "        0.20199494, 0.20199494, 0.20199494, 0.20199494, 0.20199494,\n",
              "        0.20199494, 0.20199494, 0.20199494, 0.20199494, 0.20199494,\n",
              "        0.20199494, 0.20199494, 0.20199494, 0.20199494, 0.20199494,\n",
              "        0.20199494, 0.20199494, 0.20199494, 0.20199494, 0.20199494,\n",
              "        0.20199494, 0.20199494, 0.20199494, 0.20199494, 0.20199494,\n",
              "        0.20199494, 0.20199494, 0.20199494, 0.20199494, 0.20199494,\n",
              "        0.20199494, 0.20199494, 0.20199494, 0.20199494, 0.20199494,\n",
              "        0.20199494, 0.20199494, 0.20199494, 0.20199494, 0.20199494,\n",
              "        0.20199494, 0.20199494, 0.20199494, 0.20199494, 0.20199494,\n",
              "        0.20199494, 0.20199494, 0.20199494, 0.20199494],\n",
              "       [0.06059901, 0.06059901, 0.06059901, 0.06059901, 0.06059901,\n",
              "        0.06059901, 0.06059901, 0.06059901, 0.06059901, 0.06059901,\n",
              "        0.06059901, 0.06059901, 0.06059901, 0.06059901, 0.06059901,\n",
              "        0.06059901, 0.06059901, 0.06059901, 0.06059901, 0.06059901,\n",
              "        0.06059901, 0.06059901, 0.06059901, 0.06059901, 0.06059901,\n",
              "        0.06059901, 0.06059901, 0.06059901, 0.06059901, 0.06059901,\n",
              "        0.06059901, 0.06059901, 0.06059901, 0.06059901, 0.06059901,\n",
              "        0.06059901, 0.06059901, 0.06059901, 0.06059901, 0.06059901,\n",
              "        0.06059901, 0.06059901, 0.06059901, 0.06059901, 0.06059901,\n",
              "        0.06059901, 0.06059901, 0.06059901, 0.06059901, 0.06059901,\n",
              "        0.06059901, 0.06059901, 0.06059901, 0.06059901, 0.06059901,\n",
              "        0.06059901, 0.06059901, 0.06059901, 0.06059901, 0.06059901,\n",
              "        0.06059901, 0.06059901, 0.06059901, 0.06059901, 0.06059901,\n",
              "        0.06059901, 0.06059901, 0.06059901, 0.06059901, 0.06059901,\n",
              "        0.06059901, 0.06059901, 0.06059901, 0.06059901, 0.06059901,\n",
              "        0.06059901, 0.06059901, 0.06059901, 0.06059901, 0.06059901,\n",
              "        0.06059901, 0.06059901, 0.06059901, 0.06059901, 0.06059901,\n",
              "        0.06059901, 0.06059901, 0.06059901, 0.06059901, 0.06059901,\n",
              "        0.06059901, 0.06059901, 0.06059901, 0.06059901, 0.06059901,\n",
              "        0.06059901, 0.06059901, 0.06059901, 0.06059901],\n",
              "       [0.07074469, 0.07074469, 0.07074469, 0.07074469, 0.07074469,\n",
              "        0.07074469, 0.07074469, 0.07074469, 0.07074469, 0.07074469,\n",
              "        0.07074469, 0.07074469, 0.07074469, 0.07074469, 0.07074469,\n",
              "        0.07074469, 0.07074469, 0.07074469, 0.07074469, 0.07074469,\n",
              "        0.07074469, 0.07074469, 0.07074469, 0.07074469, 0.07074469,\n",
              "        0.07074469, 0.07074469, 0.07074469, 0.07074469, 0.07074469,\n",
              "        0.07074469, 0.07074469, 0.07074469, 0.07074469, 0.07074469,\n",
              "        0.07074469, 0.07074469, 0.07074469, 0.07074469, 0.07074469,\n",
              "        0.07074469, 0.07074469, 0.07074469, 0.07074469, 0.07074469,\n",
              "        0.07074469, 0.07074469, 0.07074469, 0.07074469, 0.07074469,\n",
              "        0.07074469, 0.07074469, 0.07074469, 0.07074469, 0.07074469,\n",
              "        0.07074469, 0.07074469, 0.07074469, 0.07074469, 0.07074469,\n",
              "        0.07074469, 0.07074469, 0.07074469, 0.07074469, 0.07074469,\n",
              "        0.07074469, 0.07074469, 0.07074469, 0.07074469, 0.07074469,\n",
              "        0.07074469, 0.07074469, 0.07074469, 0.07074469, 0.07074469,\n",
              "        0.07074469, 0.07074469, 0.07074469, 0.07074469, 0.07074469,\n",
              "        0.07074469, 0.07074469, 0.07074469, 0.07074469, 0.07074469,\n",
              "        0.07074469, 0.07074469, 0.07074469, 0.07074469, 0.07074469,\n",
              "        0.07074469, 0.07074469, 0.07074469, 0.07074469, 0.07074469,\n",
              "        0.07074469, 0.07074469, 0.07074469, 0.07074469]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l33zMjZf_4NV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y=data[:,-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrHiL2M9_8_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l=[]\n",
        "for i in range(2):\n",
        "    l.append(model.ly[i].wt)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh0uwq4uA3Dw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(2):\n",
        "    model.ly[i].wt=np.array(l[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKBeHGhNBDVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
>>>>>>> c3a464b9ca1a81b3c40d7b08dd361281c3101406
    }
   ],
   "source": [
    "model.ly[-1].temp_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
